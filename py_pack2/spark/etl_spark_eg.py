# You can also use this section to suppress warnings generated by your code:
import pandas


def warn(*args, **kwargs):
    pass


import warnings

warnings.warn = warn
warnings.filterwarnings('ignore')
# FindSpark simplifies the process of using Apache Spark with Python
import findspark

findspark.init()

from pyspark.sql import SparkSession

# Create SparkSession
# Ignore any warnings by SparkSession command

spark = SparkSession.builder.appName("ETL using Spark").getOrCreate()
# create a list of tuples
# each tuple contains the student id, height and weight
data = [("student1", 64, 90),
        ("student2", 59, 100),
        ("student3", 69, 95),
        ("", 70, 110),
        ("student5", 60, 80),
        ("student3", 69, 95),
        ("student6", 62, 85),
        ("student7", 65, 80),
        ("student7", 65, 80)]

df = spark.createDataFrame(data, ["student", "height_inches", "weight_pounds"])
df.show()
spark.read.csv()
# df.write.format("csv").option("header", "true").mode("overwrite").save("file:///D:/software/workspace-py/pythonProject2/py_pack2/spark/student-hw.csv")
# 这个方法是写入hadoop的，不可以单机用
# df.write.mode("overwrite").csv(path="student-hw.csv", header=True)
df = df.toPandas()
df.to_csv(path_or_buf="student-hw.csv")
# Load student dataset
# df = spark.read.csv("file:///D:/software/workspace-py/pythonProject2/py_pack2/spark/student-hw.csv", header=True,inferSchema=True)
df = spark.read.csv("student-hw.csv", header=True, inferSchema=True)

# display dataframe
df.show()
