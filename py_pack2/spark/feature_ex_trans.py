# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass


import warnings

warnings.warn = warn
warnings.filterwarnings('ignore')

# FindSpark simplifies the process of using Apache Spark with Python

import findspark

findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import rand

spark = SparkSession.builder.appName("Feature Extraction and Transformation using Spark").getOrCreate()
# import tokenizer
from pyspark.ml.feature import Tokenizer

# create a sample dataframe
sentenceDataFrame = spark.createDataFrame([
    (1, "Spark is a distributed computing system."),
    (2, "It provides interfaces for multiple languages"),
    (3, "Spark is built on top of Hadoop")
], ["id", "sentence"])
# display the dataframe
sentenceDataFrame.show(truncate=False)

# create tokenizer instance.
# mention the column to be tokenized as inputcol
# mention the output column name where the tokens are to be stored.
tokenizer = Tokenizer(inputCol="sentence", outputCol="words")
token_df = tokenizer.transform(sentenceDataFrame)
# display the tokenized data
token_df.show(truncate=False)

# import CountVectorizer
from pyspark.ml.feature import CountVectorizer

# create a sample dataframe and display it.
textdata = [(1, "I love Spark Spark provides Python API ".split()),
            (2, "I love Python Spark supports Python".split()),
            (3, "Spark solves the big problem of big data".split())]

textdata = spark.createDataFrame(textdata, ["id", "words"])

textdata.show(truncate=False)
# Create a CountVectorizer object
# mention the column to be count vectorized as inputcol
# mention the output column name where the count vectors are to be stored.
### 词频向量
cv = CountVectorizer(inputCol="words", outputCol="features")
# Fit the CountVectorizer model on the input data
model = cv.fit(textdata)
# Transform the input data to bag-of-words vectors
result = model.transform(textdata)
result.show(truncate=False)
####TF-IDF
from pyspark.ml.feature import HashingTF, IDF, Tokenizer

# create a sample dataframe and display it.
sentenceData = spark.createDataFrame([
    (1, "Spark supports python"),
    (2, "Spark is fast"),
    (3, "Spark is easy")
], ["id", "sentence"])

sentenceData.show(truncate=False)
# tokenize the "sentence" column and store in the column "words"
tokenizer = Tokenizer(inputCol="sentence", outputCol="words")
wordsData = tokenizer.transform(sentenceData)
wordsData.show(truncate=False)
# Create a HashingTF object （词频-逆文档频率），是一种用于评估一个词在文档集合中的重要性的统计方法
# mention the "words" column as input
# mention the "rawFeatures" column as output

hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=10)
featurizedData = hashingTF.transform(wordsData)

featurizedData.show(truncate=False)
# Create an IDF object
# mention the "rawFeatures" column as input
# mention the "features" column as output

idf = IDF(inputCol="rawFeatures", outputCol="features")
idfModel = idf.fit(featurizedData)
tfidfData = idfModel.transform(featurizedData)
# display the tf-idf data
tfidfData.select("sentence", "features").show(truncate=False)

# StopWordsRemover is a transformer that filters out stop words like "a","an" and "the"
# import StopWordsRemover
from pyspark.ml.feature import StopWordsRemover

# create a dataframe with sample text and display it
textData = spark.createDataFrame([
    (1, ['Spark', 'is', 'an', 'open-source', 'distributed', 'computing', 'system']),
    (2, ['IT', 'has', 'interfaces', 'for', 'multiple', 'languages']),
    (3, ['It', 'has', 'a', 'wide', 'range', 'of', 'libraries', 'and', 'APIs'])
], ["id", "sentence"])

textData.show(truncate=False)
# remove stopwords from "sentence" column and store the result in "filtered_sentence" column
remover = StopWordsRemover(inputCol="sentence", outputCol="filtered_sentence")
textData = remover.transform(textData)
textData.show(truncate=False)
# StringIndexer converts a column of strings into a column of integers.
from pyspark.ml.feature import StringIndexer

colors = spark.createDataFrame(
    [(0, "red"), (1, "red"), (2, "blue"), (3, "yellow"), (4, "yellow"), (5, "yellow")],
    ["id", "color"])

colors.show()
# index the strings in the column "color" and store their indexes in the column "colorIndex"
indexer = StringIndexer(inputCol="color", outputCol="colorIndex")
indexed = indexer.fit(colors).transform(colors)
indexed.show()

# StandardScaler transforms the data so that it has a mean of 0 and a standard deviation of 1
# 特征转换器，用于将数据集中的特征缩放为标准正态分布
# import StandardScaler
from pyspark.ml.feature import StandardScaler
# Create a sample dataframe and display it
from pyspark.ml.linalg import Vectors

data = [(1, Vectors.dense([70, 170, 17])),
        (2, Vectors.dense([80, 165, 25])),
        (3, Vectors.dense([65, 150, 135]))]
df = spark.createDataFrame(data, ["id", "features"])

df.show()
# Define the StandardScaler transformer
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withStd=True, withMean=True)
# Fit the transformer to the dataset
scalerModel = scaler.fit(df)
scaledData = scalerModel.transform(df)
scaledData.show(truncate=False)
spark.stop()
