

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# FindSpark simplifies the process of using Apache Spark with Python

import findspark
findspark.init()

#import functions/Classes for sparkml

from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Clustering using SparkML").getOrCreate()
customer_data = spark.read.csv("customers.csv", header=True, inferSchema=True)
customer_data.printSchema()
customer_data.show(n=5, truncate=False)
# Assemble the features into a single vector column
feature_cols = ['Fresh_Food', 'Milk', 'Grocery', 'Frozen_Food']
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
customer_transformed_data = assembler.transform(customer_data)
number_of_clusters = 3
#Create a KMeans clustering model

kmeans = KMeans(k = number_of_clusters)
model = kmeans.fit(customer_transformed_data)
# Make predictions on the dataset
predictions = model.transform(customer_transformed_data)
predictions.show(5)
predictions.groupBy('prediction').count().show()
#stop spark session
spark.stop()
