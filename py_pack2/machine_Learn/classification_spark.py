# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass


import warnings

warnings.warn = warn
warnings.filterwarnings('ignore')

# FindSpark simplifies the process of using Apache Spark with Python

import findspark

findspark.init()

from pyspark.sql import SparkSession

# import functions/Classes for sparkml

from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression

# import functions/Classes for metrics
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

spark = SparkSession.builder.appName("Classification using SparkML").getOrCreate()
beans_data = spark.read.csv("drybeans.csv", header=True, inferSchema=True)
beans_data.printSchema()
beans_data.select(["Area", "Perimeter", "Solidity", "roundness", "Compactness", "Class"]).show(5)
beans_data.groupBy('Class').count().orderBy('count').show()
# Convert Class column from string to numerical values
indexer = StringIndexer(inputCol="Class", outputCol="label")
beans_data = indexer.fit(beans_data).transform(beans_data)
beans_data.groupBy('label').count().orderBy('count').show()
### Task 3 - Identify the label column and the input columns
assembler = VectorAssembler(inputCols=["Area", "Perimeter", "Solidity", "roundness", "Compactness"],
                            outputCol="features")
beans_transformed_data = assembler.transform(beans_data)
beans_transformed_data.select("features", "label").show()
(training_data, testing_data) = beans_transformed_data.randomSplit([0.7, 0.3], seed=42)
lr = LogisticRegression(featuresCol="features", labelCol="label")
model = lr.fit(training_data)
# Make predictions on testing data
predictions = model.transform(testing_data)
#Evaluate the data
# Evaluate model performance
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Accuracy =", accuracy)
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedPrecision")
precision = evaluator.evaluate(predictions)
print("Precision =", precision)
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedRecall")
recall = evaluator.evaluate(predictions)
print("Recall =", recall)
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")
f1_score = evaluator.evaluate(predictions)
print("F1 score = ", f1_score)
spark.stop()
